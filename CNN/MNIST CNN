#Running a CNN on MNIST data

#In progress, incomplete. Check back shortly

#Make sure you have first gone through the ANN model on MNIST Data before starting with CNN.
#Make sure you have good understanding of how kernels work too.

_______________________
Structure :
0) Input - MNIST datas
1) Convolutional and Max-Pooling
2) Convolutional and Max-Pooling
3) Fully Connected Layer
4) Processing - Dropout 
5) Readout layer - Fully Connected
6) Outputs - Classified digits
_________________________




import tensorflow as tf

# finish possible remaining sessions
sess.close()

#Start interactive session
sess = tf.InteractiveSession()


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)


width = 28 # width of the image in pixels 
height = 28 # height of the image in pixels
flat = width * height # number of pixels in one image 
class_output = 10 # number of possible classifications for the problem


x  = tf.placeholder(tf.float32, shape=[None, flat])
y_ = tf.placeholder(tf.float32, shape=[None, class_output])

# function to generate weights, 

# function to generate biases, please note, neurons are sensitive during initial stages, so please use atleast slightly positive values




#Converting images of the data set to tensors
#The input image is a 28 pixels by 28 pixels and 1 channel (grayscale)
#In this case the first dimension is the batch number of the image (position of the input on the batch)
and can be of any size (due to -1)

x_image = tf.reshape(x, [-1,28,28,1])  

x_image


Convolutional Layer 1 :
Defining kernel weight and bias
Size of the filter/kernel: 5x5;
Input channels: 1 (greyscale);
32 feature maps 
(here, 32 feature maps means 32 different filters are applied on each image. 
So, the output of convolution layer would be 28x28x32).
In this step, we create a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]

        W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))
        b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) # need 32 biases for 32 outputs
        
        
Now do a Convolution with weight tensor and add biases.
To creat convolutional layer, we use tf.nn.conv2d. 

This computes a 2-D convolution given 4-D input and filter tensors.

Inputs:
tensor of shape [batch, in_height, in_width, in_channels]. x of shape [batch_size,28 ,28, 1]
a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]. 
W is of size [5, 5, 1, 32]
stride which is [1, 1, 1, 1]

Process:
change the filter to a 2-D matrix with shape [5*5*1,32]
Extracts image patches from the input tensor to form a virtual tensor of shape [batch, 28, 28, 5*5*1].
For each patch, right-multiplies the filter matrix and the image patch vector.

Output:
A Tensor (a 2-D convolution) of size <tf.Tensor 'add_7:0' shape=(?, 28, 28, 32)- 
Notice: the output of the first convolution layer is 32 [28x28] images. 
Here 32 is considered as volume/depth of the output image.        

So combining everything we arrive at :

        convolve1= tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1
        
        
Apply Relu 
      h_conv1 = tf.nn.relu(convolve1)
      

Apply the max pooling
Use the max pooling operation already defined, so the output would be 14x14x32
Defining a function to perform max pooling. The maximum pooling is an operation that finds maximum values and simplifies the inputs using the spacial correlations between them.
Kernel size: 2x2 (if the window is a 2x2 matrix, it would result in one output pixel)
Strides: dictates the sliding behaviour of the kernel. In this case it will move 2 pixels everytime, thus not overlapping.


        h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #max_pool_2x2
        
        layer1= h_pool1
        
        #End of Layer 1.
        
        
        
        

